{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhamLeQuangNhat/CS114.K21.KHTN/blob/master/Sarcasm/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNrv8WOQjqkY",
        "colab_type": "text"
      },
      "source": [
        "**Danh sách nhóm**: \n",
        "+ Phạm Xuân Trí  - 18521530 \n",
        "+ Phạm Lê Quang Nhật - 18520120"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4gbro_VFtRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "f3593ca3-36e5-4be0-b43d-9ebd5b5f159f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0glhxbOZF4kW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5dec5d5-a7e7-471e-fb00-15e32587542f"
      },
      "source": [
        "cd drive/My Drive/ML/Demo "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ML/Demo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zom5_zJPF86O",
        "colab_type": "text"
      },
      "source": [
        "#**1. Mô tả lại bài toán và cách thu thập dataset**\n",
        "Bài toán **Sacrasm Detection** đây là bài toán phát hiện có tính châm biến trong một câu văn.\n",
        "* Input: Một câu văn \n",
        "* Ouput: Câu văn được phân lớp là 1 nếu là câu châm biến, ngược lại nó được phân lớp là 0.\n",
        "\n",
        "Cụ thể trong bài toán này chúng em dựa vào những headline của các bài báo điện tử trên trang **\"The Onion\"** và **\"HuffPost\"** để xây dựng mô hình dự đoán. Trang \"The Onion\" là trang chỉ chứa các headline là câu châm biến. Ngược lại với \"The Onion\", \"HuffPost\" là trang chỉ chứa các headline không phải là câu châm biến. \n",
        "\n",
        "**Cách thu thập dữ liệu**\n",
        "* Dữ liệu huấn luyện: Dữ liệu được download trên trang [kaggle](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection) được lưu trong file.json. Mỗi dòng dữ liệu bao gồm: \n",
        "     * **article**: Link dẫn tới bài báo tương ứng\n",
        "     * **headline**: Tiêu đề của bài báo cần phân loại\n",
        "     * **is sacrasm**: Tiêu đề có phải là châm biến hay không. Nếu có mang giá trị 1, ngược lại là 0\n",
        "     \n",
        "Có tất cả **28619** dòng dữ liệu, với **x** dòng là châm biến, còn lại **y** dòng là không châm biến bộ dữ liệu này được trích ra 75% để huấn luyện.  \n",
        "* Dữ liệu kiểm thử\n",
        "    +  Lấy 25% từ bộ dữ liệu download đã nêu trên\n",
        "* Dữ liệu test\n",
        "    +  Sẽ tiến hành crawl trên 2 trang báo điện tử ở trên. Tổng cộng có 2000 headline mới. Mỗi trang 1000 headline \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbUOJKAUGJxD",
        "colab_type": "text"
      },
      "source": [
        "#**2.Thu thập dữ liệu mới**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aoiWqFfGKTi",
        "colab_type": "text"
      },
      "source": [
        "#### **Dữ liệu mới từ Onion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkNCqBNvGYw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "'''  \n",
        " brief_ids và photo_ids là các id tưng ứng của  dẫn tới link các bài báo khác nhau\n",
        "\n",
        "'''\n",
        "brief_ids = [1592311200653, 1591726080182, 1591127280478, 1590584520209, \n",
        "             1589894280339, 1589294340828, 1588711020164, 1588080060229, \n",
        "             1587147720532, 1586546460312, 1586184540217, 1585581600531,\n",
        "             1582920840434, 1582639200695, 1582128720515, 1581532200479,\n",
        "             1581099360048, 1580744340055, 1580142900840, 1579637820343,\n",
        "             1579096620327, 1578410280620, 1576700820848, 1576184280994,\n",
        "             1575574260415, 1575036000486, 1574361720213, 1573762560848,\n",
        "             1573477200438, 1572900480129, 1572348600499, 1571938620032,\n",
        "             1571419440061]\n",
        "\n",
        "photo_ids = [1590159060816, 1588708260098, 1587653040878, 1585925400900,\n",
        "             1584127800188, 1582815000473, 1582061760395, 1580835900882,\n",
        "             1579528800910, 1576610760393, 1574702400578, 1573153500106,\n",
        "             1571849700244, 1570194000584, 1569247200599, 1567182180912,\n",
        "             1565892720386, 1564614900603, 1563235200067, 1560963960321,\n",
        "             1559133180370, 1557413700296, 1556108520577, 1554495180638,\n",
        "             1552834800968, 1551123540512, 1550160960071, 1548972300054,\n",
        "             1547739780965, 1544713860139, 1543513440446, 1541701980758,\n",
        "             1539720900721]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-2ugzzMGZxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Onion_crawler(tag, ids, num_articles, label=1):\n",
        "    count = 0\n",
        "    for id in ids:\n",
        "      source = requests.get(\"https://www.theonion.com/c/{}?startTime={}\".format(tag,id)).text\n",
        "      soup = BeautifulSoup(source, 'lxml')\n",
        "      for article in soup.find_all('article'):\n",
        "          headline = article.h2.text\n",
        "          csv_writer.writerow([label, headline])\n",
        "          count += 1\n",
        "          if count == num_articles:\n",
        "              print(f\"Crawled {count} headlines from {tag} tag\")\n",
        "              return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QM8I7xUGeET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d579c9e-3b96-4415-b220-1a425b7b5628"
      },
      "source": [
        "csv_file = open('onion_headlines.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['is_sarcastic', 'headline'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNsW8-ODGhZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "57e91f08-dab5-4460-9ce4-6bc73fb59d6a"
      },
      "source": [
        "Onion_crawler(\"news-in-brief\", brief_ids, num_articles=644, label=1)\n",
        "Onion_crawler(\"news-in-photos\", photo_ids, num_articles=643, label=1)\n",
        "csv_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crawled 644 headlines from news-in-brief tag\n",
            "Crawled 643 headlines from news-in-photos tag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCu6AE7AGmbm",
        "colab_type": "text"
      },
      "source": [
        "#### **Dữ liệu mới từ Huffpost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9oFEv2UGsF9",
        "colab_type": "text"
      },
      "source": [
        "Ta lấy data từ API của HuffPost.\n",
        "\n",
        "- Đầu tiên gửi request lấy [500 bài báo đầu tiên](https://www.huffpost.com/api/department/news/cards?page=1&limit=500) từ API. Tại đây ta copy toàn bộ file json và lưu vào file huffpost_part1.json.\n",
        "-  Tiếp theo ta gửi request lấy [500 bài báo tiếp theo](https://www.huffpost.com/api/department/news/cards?page=2&limit=500) từ API. Tuy nhiên lần này API chỉ trả về 194 bài báo. HuffPost API chỉ cho ta request 694 bài báo gần đây. Ta copy file json và lưu vào file huffpost_part2.json.\n",
        "    chen hinh here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqcLVXzLGvIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jir30mnJGwoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('part1.json') as f:\n",
        "    part1 = json.load(f)\n",
        "\n",
        "with open('part2.json') as f:\n",
        "    part2 = json.load(f)\n",
        "\n",
        "csv_file = open('huffpost_headlines.csv', 'w')\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['is_sarcastic', 'headline'])\n",
        "\n",
        "for article in part1[\"cards\"]:\n",
        "    headline = article[\"headlines\"][0][\"text\"]\n",
        "    csv_writer.writerow([0, headline])\n",
        "    \n",
        "for article in part2[\"cards\"]:\n",
        "    headline = article[\"headlines\"][0][\"text\"]\n",
        "    csv_writer.writerow([0, headline])\n",
        "\n",
        "csv_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4z6GbAkG09y",
        "colab_type": "text"
      },
      "source": [
        "#**3. Mô tả cách xử lý dữ liệu, feature engineering trên dataset đã cho.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "392cT_z2G3dc",
        "colab_type": "text"
      },
      "source": [
        "####  Cài đặt các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhd2h8FSGyzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re,string"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2os_t8XS0-kx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f0bf3115-8efb-4809-a576-e8fab589a289"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final.ipynb\t\tpart1.json\t       Sarcasm.ipynb\n",
            "huffpost_headlines.csv\tpart2.json\n",
            "onion_headlines.csv\tSarcasmDatasetv2.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKQM8WGwHwDl",
        "colab_type": "text"
      },
      "source": [
        "#### Tải dữ liệu train và phân chia thành 2 phần: tập validation và tập train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WIQ6W9p2Uth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = pd.read_json(\"SarcasmDatasetv2.json\",lines=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "notRAn4V03m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_train['headline']\n",
        "Y = data_train['is_sarcastic']\n",
        "\n",
        "#split validation:25%, train:75%\n",
        "trainX,valX,trainY,valY = train_test_split(X,Y,test_size=0.25,random_state=42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD31XdqyI8rb",
        "colab_type": "text"
      },
      "source": [
        "#### Tải dữ liệu test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-V0R-M9I_Xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "onion = pd.read_csv(\"onion_headlines.csv\")\n",
        "onion_X = onion['headline']\n",
        "onion_Y = onion['is_sarcastic']\n",
        "\n",
        "huffpost = pd.read_csv(\"huffpost_headlines.csv\")\n",
        "huffpost_X = huffpost['headline']\n",
        "huffpost_Y = huffpost['is_sarcastic']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt94WfeMJC__",
        "colab_type": "text"
      },
      "source": [
        "#### Tiền xử lí dữ liệu \n",
        "- Các headline sẽ được chuyển về lowercase, loại bỏ các ký tự đặc biệt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-cPdwX-JKwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DN2WCH8JNi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainX = trainX.apply(clean_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RF-xO55JP6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valX = valX.apply(clean_text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLhp2aNoJSOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "onion_X = onion_X.apply(clean_text)\n",
        "huffpost_X = huffpost_X.apply(clean_text)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n4kBzkbJWOS",
        "colab_type": "text"
      },
      "source": [
        "#### Feature engineering cho bộ data train,test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROe7ozgiJjZo",
        "colab_type": "text"
      },
      "source": [
        "- Sử dụng TfidfVectorizer để trích xuất đặc trưng.\n",
        "- Khai báo TfidfVectorizer từ thư viện sklearn để biểu diễn dữ liệu train dưới dạng vector và tạo một bộ từ điển từ dữ liệu train\n",
        "- Sử dụng lại từ điển của dữ liệu train để biểu diễn dữ liệu validation, test dưới dạng vector\n",
        "*Hàm TfidfTransformer để đánh giá lại về giá trị các từ bởi vì có những từ tuy xuất hiện nhiều lần nhưng nó lại thực sự không quan trọng (the, and, is,of,that,...)=> giúp model đạt kết quả chính xác hơn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kmLJ3jFu9_Z",
        "colab_type": "text"
      },
      "source": [
        "Giải thích về cách tính Tfidf:\n",
        "\n",
        "*Tfidf = Tf * idf,với:\n",
        "\n",
        "-Tf: tần số xuất hiện của một từ trong văn bản đó = Số lần xuất hiện của từ nhất định / Số lần xuất hiện nhiều nhất của một từ bất kỳ\n",
        "\n",
        "-idf: nghịch đảo tần số của văn bản chứa từ nhất định = log(Tổng số văn bản / số văn bản chứa từ đó+1)\n",
        "\n",
        "Những từ không quan trọng (the,and,it,is,...) thường xuất hiện nhiều trong các văn bản cho nên theo công thức trên ta thấy số văn bản chứa từ đó càng nhiều thì idf càng thấp => tfidf càng thấp => tầm quan trọng của từ đó thấp\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ0prjaIJVeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dde03a8-261c-447b-c298-60609536ee66"
      },
      "source": [
        "# khởi tạo TfidfVectorizer\n",
        "tf = TfidfVectorizer(ngram_range=(1,2), max_features=40000, min_df=2)\n",
        "\n",
        "# chuyển words sang vetor\n",
        "trainX = tf.fit_transform(trainX.values).toarray()\n",
        "trainVocab = tf.vocabulary_ \n",
        "tf = TfidfVectorizer(vocabulary=trainVocab)\n",
        "\n",
        "valX = tf.fit_transform(valX.values).toarray()\n",
        "\n",
        "onion_X = tf.fit_transform(onion_X.values).toarray()\n",
        "huffpost_X = tf.fit_transform(huffpost_X.values).toarray()\n",
        "testX = np.concatenate((onion_X,huffpost_X),axis=0)\n",
        "testY = np.concatenate((onion_Y,huffpost_Y),axis=0)\n",
        "print(\"[INFO] Used TfidfVectorizer ... \")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Used TfidfVectorizer ... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5bVtXQvJb8W",
        "colab_type": "text"
      },
      "source": [
        "#**4. Mô tả quá trình chọn model, học và fine tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ys3ywhYJt8V",
        "colab_type": "text"
      },
      "source": [
        "- Vì đây là bài toán phân lớp nên ta sẽ chọn những thuật toán phân lớp như Suport vector classifications,Multinomial Naive Bayes, Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-IcP_EUJ2Ad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = LogisticRegression()\n",
        "model2 = MultinomialNB()\n",
        "model3 = LinearSVC()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXM2n2B5J6q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "9e3e32bb-b8a9-4ee3-baba-b748a9b991fa"
      },
      "source": [
        "for i in range(3):\n",
        "    \n",
        "    if i == 0:\n",
        "\n",
        "        print(\"   [INFO] evaluating Logistic Regression...\")\n",
        "\n",
        "        # train and evaluating \n",
        "        model1.fit(trainX, trainY)\n",
        "        predict_val_1 = model1.predict(valX)\n",
        "        predict_test_1 = model1.predict(testX)\n",
        "\n",
        "        print(classification_report(valY,predict_val_1))\n",
        "\n",
        "    if i == 1:\n",
        "        \n",
        "        print(\"   [INFO] evaluating Naive Bayes...\")\n",
        "\n",
        "        # train and evaluating \n",
        "        model2.fit(trainX, trainY)\n",
        "        predict_val_2 = model2.predict(valX)\n",
        "        predict_test_2 = model2.predict(testX)\n",
        "\n",
        "        print(classification_report(valY,predict_val_2))\n",
        "\n",
        "    if i == 2:\n",
        "\n",
        "        print(\"   [INFO] evaluating SVM...\")\n",
        "\n",
        "        model3.fit(trainX, trainY)\n",
        "        predict_val_3 = model3.predict(valX)\n",
        "        predict_test_3 = model3.predict(testX)\n",
        "\n",
        "        print(classification_report(valY,predict_val_3))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   [INFO] evaluating Logistic Regression...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83      3730\n",
            "           1       0.80      0.84      0.82      3425\n",
            "\n",
            "    accuracy                           0.82      7155\n",
            "   macro avg       0.82      0.83      0.82      7155\n",
            "weighted avg       0.83      0.82      0.82      7155\n",
            "\n",
            "   [INFO] evaluating Naive Bayes...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85      3730\n",
            "           1       0.86      0.79      0.82      3425\n",
            "\n",
            "    accuracy                           0.83      7155\n",
            "   macro avg       0.84      0.83      0.83      7155\n",
            "weighted avg       0.84      0.83      0.83      7155\n",
            "\n",
            "   [INFO] evaluating SVM...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.83      0.83      3730\n",
            "           1       0.82      0.82      0.82      3425\n",
            "\n",
            "    accuracy                           0.83      7155\n",
            "   macro avg       0.83      0.83      0.83      7155\n",
            "weighted avg       0.83      0.83      0.83      7155\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmSAa0ThKqRb",
        "colab_type": "text"
      },
      "source": [
        "#**5. Mô tả cách dùng model đã train để viết một đoạn chương trình ngắn, thực hiện sacarsm detection cho một headline bất kỳ được nhập vào.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rByUDQNKtZv",
        "colab_type": "text"
      },
      "source": [
        "#### Nhập một headline bất kỳ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixN7q-rXKyMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88ad2bae-6101-4dbb-fe43-97e6a998984b"
      },
      "source": [
        "headline = input()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crowds Tear Down Statues At Wisconsin Capitol, Attack State Senator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SydbuE_KKwhW",
        "colab_type": "text"
      },
      "source": [
        "#### Tiền xử lí dữ liệu \n",
        "- Headline sẽ được chuyển về lowercase, loại bỏ các ký tự đặc biệt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhKo-k7rK-cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline = [clean_text(headline)]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n77UetLLAOP",
        "colab_type": "text"
      },
      "source": [
        "#### Mã hoá headline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pD7Or59LGqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline = tf.fit_transform(headline).toarray()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQKbvkVNLWzk",
        "colab_type": "text"
      },
      "source": [
        "#### Dự đoán"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP2rIdCDLY_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = model2.predict(headline)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX70fUWHLjBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea342ccf-d20b-4adb-8c7e-dfc01e02f4fb"
      },
      "source": [
        "if predict == 1:\n",
        "    print(\"This is sarcastic_from the onion\")\n",
        "else:\n",
        "    print(\"This isn't sarcastic_from the huffpost\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This isn't sarcastic_from the huffpost\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4o9lgrUL5EN",
        "colab_type": "text"
      },
      "source": [
        "#**6. Đối chiếu performance của model trên dataset đã cho và trên 2000 headine mới. Nhận xét về bài toán này.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3bqjqTWMNRi",
        "colab_type": "text"
      },
      "source": [
        "#### Đánh giá mô hình Multinomial Naive Bayes trên dataset đã cho và trên 2000 headine mới."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6i00HkCMEPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "18d9902c-934a-4267-ae1b-c1cc9f4663fc"
      },
      "source": [
        "print(classification_report(valY,predict_val_2))\n",
        "print(classification_report(testY,predict_test_2))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.88      0.85      3730\n",
            "           1       0.86      0.79      0.82      3425\n",
            "\n",
            "    accuracy                           0.83      7155\n",
            "   macro avg       0.84      0.83      0.83      7155\n",
            "weighted avg       0.84      0.83      0.83      7155\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.83      0.73       694\n",
            "           1       0.89      0.75      0.82      1287\n",
            "\n",
            "    accuracy                           0.78      1981\n",
            "   macro avg       0.77      0.79      0.77      1981\n",
            "weighted avg       0.81      0.78      0.79      1981\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amRMPPtJL8v6",
        "colab_type": "text"
      },
      "source": [
        "#### Nhận xét"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIT23S26wNN-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Hiệu suất sụt giảm so với bộ dữ liệu cũ vì có những tin tức mới hay nhiều word mới có giá trị TFIDF lớn liên quan đến bài toán không có trong bộ từ vựng đã xây dựng  => sai lệch => giảm hiệu suất\n",
        "\n",
        "Nhận xét về nhóm bài toán này: Đây là bài toán khó, không thể đạt độ chính xác cao nếu chỉ sử dụng machine learning vì tin tức mỗi ngày mỗi khác, mỗi đổi mới nếu không cập nhật các tin tức mà chỉ sử dụng các bag of words cũ đã được thu thập từ trước thì sẽ thiếu hụt các từ có giá trị cao(weight).\n",
        "\n",
        "Giải pháp: nên sử dụng deep learning và thực hiện mã hoá bộ từ vựng theo phương pháp word2vec để giải quyết bài toán này\n"
      ]
    }
  ]
}